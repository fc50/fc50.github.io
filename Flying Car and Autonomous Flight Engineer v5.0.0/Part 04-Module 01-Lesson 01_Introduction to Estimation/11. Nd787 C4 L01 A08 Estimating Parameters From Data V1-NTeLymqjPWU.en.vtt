WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.609
One situation where we want to do

00:00:02.609 --> 00:00:06.389
parameter estimation has been coming up with a model for a sensor.

00:00:06.389 --> 00:00:07.955
Take a laser rangefinder.

00:00:07.955 --> 00:00:11.370
If you mount a laser rangefinder exactly one meter from a wall,

00:00:11.369 --> 00:00:13.229
and use it to measure the distance,

00:00:13.230 --> 00:00:16.420
you probably won't get a measure distance of exactly one meter.

00:00:16.420 --> 00:00:19.650
It will likely be slightly above, or slightly below.

00:00:19.649 --> 00:00:23.009
But if you repeat this measurement many times,

00:00:23.010 --> 00:00:25.950
you'll likely find that on average the sensor does

00:00:25.949 --> 00:00:28.980
measure the correct distance plus or minus some error.

00:00:28.980 --> 00:00:31.289
That error is also called sensor noise,

00:00:31.289 --> 00:00:36.704
and it's often best modeled as a Gaussian random variable with some mean ideally zero,

00:00:36.704 --> 00:00:38.039
and some standard deviation.

00:00:38.039 --> 00:00:41.609
In order to use that sensor most effectively,

00:00:41.609 --> 00:00:46.244
we need an accurate estimate of what the parameters of that distribution are.

00:00:46.244 --> 00:00:52.004
We can't know the exact values of the parameters of a distribution given some data,

00:00:52.005 --> 00:00:56.130
but we can compute estimates of the parameters in a variety of ways.

00:00:56.130 --> 00:00:59.010
For example, a Gaussian is parameterized, as

00:00:59.009 --> 00:01:01.905
we said a minute ago, by mu and sigma squared.

00:01:01.905 --> 00:01:06.254
We'll write the estimates of these parameters as x_hat, and sigma_hat squared.

00:01:06.254 --> 00:01:10.964
Now, let's imagine that I have some Gaussian that's generating samples from x,

00:01:10.965 --> 00:01:14.219
and this is p of x, and we don't know mu, or sigma.

00:01:14.219 --> 00:01:18.000
We repeatedly draw from this distribution to collect samples of data.

00:01:18.000 --> 00:01:22.519
And you can see we'll likely get more data here near the mean,

00:01:22.519 --> 00:01:24.774
and fewer samples out on the tails.

00:01:24.775 --> 00:01:27.660
Using a technique called the method of moments,

00:01:27.659 --> 00:01:30.420
it's easy to show that if we have samples x_1,

00:01:30.420 --> 00:01:32.234
x_2 up to x_n,

00:01:32.234 --> 00:01:34.560
then x_hat is equal to one over n,

00:01:34.560 --> 00:01:38.040
sum from i equals one to n of x of i.

00:01:38.040 --> 00:01:40.950
And sigma_hat squared is equal to one over n,

00:01:40.950 --> 00:01:43.635
sum from i equals one to n of x_sub_i,

00:01:43.635 --> 00:01:45.825
minus x_hat all squared.

00:01:45.825 --> 00:01:50.450
It's quite convenient that the mean of the distribution matches the expected value,

00:01:50.450 --> 00:01:53.840
and the variance of the distribution matches that variance.

