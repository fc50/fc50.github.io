WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.490
At this point, we figured out how we can take

00:00:02.490 --> 00:00:04.769
a set of measurements that are linear function of

00:00:04.769 --> 00:00:09.494
the unknown quantity to be estimated the state and solve it as a least squares problem,

00:00:09.494 --> 00:00:11.474
but we had to make some assumptions.

00:00:11.474 --> 00:00:14.504
Let's see if we can relax one of those assumptions.

00:00:14.505 --> 00:00:17.775
Let's go back to our third experiment of a flying car

00:00:17.774 --> 00:00:21.214
receiving measurements that are linear function of the state.

00:00:21.214 --> 00:00:24.539
We're going to still assume that the vehicle is not moving and we're

00:00:24.539 --> 00:00:27.824
going to assume the measurements are noisy but linear function of the position.

00:00:27.824 --> 00:00:31.939
But now, let's allow the measurements to actually be received one at a time.

00:00:31.940 --> 00:00:34.910
So, instead of coming up with an estimate all once,

00:00:34.909 --> 00:00:37.069
we're going to think about maintaining an estimate,

00:00:37.070 --> 00:00:39.840
and allowing new measurements to update that estimate.

00:00:39.840 --> 00:00:45.085
Once again, we'll make m measurements y of our constant vector x,

00:00:45.085 --> 00:00:49.594
such that y is equal to H times x plus v. In this equation,

00:00:49.594 --> 00:00:52.475
x is again the constant unknown state vector of length n,

00:00:52.475 --> 00:00:53.870
the thing we want to estimate,

00:00:53.869 --> 00:00:57.199
y tilde is the vector of actual measurements received.

00:00:57.200 --> 00:01:00.080
V is the unknown error vector and H is again

00:01:00.079 --> 00:01:03.820
the measurement matrix assumed to be constant and known.

00:01:03.820 --> 00:01:09.555
Let's assume that we now have a prior estimate of the vehicle position,

00:01:09.555 --> 00:01:11.520
we're going to assume that it's given as

00:01:11.519 --> 00:01:15.060
a Gaussian and it's going to have an initial estimate x naught,

00:01:15.060 --> 00:01:17.144
with some co-variance Q naught.

00:01:17.144 --> 00:01:21.659
I am going to use the subscript naught to denote the estimate before any measurement is

00:01:21.659 --> 00:01:26.924
received and subscript one to denote the estimate after one measurement is received,

00:01:26.924 --> 00:01:29.325
and will increment from there.

00:01:29.325 --> 00:01:33.450
How can we actually use this prior in our estimation process?

00:01:33.450 --> 00:01:36.060
We're going to do a very similar approach to

00:01:36.060 --> 00:01:39.344
the two steps we did before maximum likelihood estimation.

00:01:39.344 --> 00:01:42.704
Remember that with maximum likelihood estimation,

00:01:42.704 --> 00:01:46.924
we first found the probability density function p of y given x,

00:01:46.924 --> 00:01:49.310
and then we selected the x to be the value

00:01:49.310 --> 00:01:51.799
that yielded the maximum likelihood of the measurements,

00:01:51.799 --> 00:01:54.384
y tilde given that x hat.

00:01:54.385 --> 00:01:57.725
The difference is that now we have a prior,

00:01:57.724 --> 00:02:02.239
we can find the posterior estimate p of x, given y tilde.

00:02:02.239 --> 00:02:05.375
The two steps of this new process are, one,

00:02:05.375 --> 00:02:09.280
can we find the probability density function p of x given y?

00:02:09.280 --> 00:02:12.469
Two, can we select x hat to be the value that

00:02:12.469 --> 00:02:15.965
yields the maximum p of x hat, given y tilde?

00:02:15.965 --> 00:02:19.319
The output of this recursive estimate is usually referred

00:02:19.319 --> 00:02:22.824
to as the maximum A posteriori or MAP estimate.

00:02:22.824 --> 00:02:26.579
We're taking our prior and turning it into a posterior using

00:02:26.580 --> 00:02:30.885
new information from the new measurement. How do we do this?

00:02:30.884 --> 00:02:35.745
Well, the first step is to find the PDF of p of x given y.

00:02:35.745 --> 00:02:40.259
Remember, from Bayes rule applied to Gaussians that we can turn p of x given

00:02:40.259 --> 00:02:44.984
y into p of y given x times p of x over p of y.

00:02:44.985 --> 00:02:49.200
That's equal to some normalizer alpha times a normal distribution with

00:02:49.199 --> 00:02:53.354
mean H times x and variance R. This is p of y given x,

00:02:53.354 --> 00:02:55.379
we worked that out in the previous concepts.

00:02:55.379 --> 00:02:58.590
We then multiply that by p of x which is our prior,

00:02:58.590 --> 00:03:01.019
which remember is given to us as a normal distribution

00:03:01.019 --> 00:03:04.425
with mean x hat naught and co-variance Q naught.

00:03:04.425 --> 00:03:09.960
We can ignore the normalizer alpha just as we did in the maximum likelihood estimate,

00:03:09.960 --> 00:03:13.920
the normalizer is not going to have any impact on the maximum.

00:03:13.919 --> 00:03:18.169
The next thing is, we select x hat to be the value that

00:03:18.169 --> 00:03:22.224
yields a maximum likelihood of p of x hat given y tilde.

00:03:22.224 --> 00:03:25.639
If we look at the full formula for the product Gaussian,

00:03:25.639 --> 00:03:28.369
we have two of the normalizers from the Gaussian here,

00:03:28.370 --> 00:03:31.414
and then we actually have the product of the two exponents,

00:03:31.414 --> 00:03:34.914
which when you take the product of exponents becomes the exponent of the sum.

00:03:34.914 --> 00:03:38.959
Just as before, we see this expression consists of a normalizer term up front

00:03:38.960 --> 00:03:43.219
which does not depend on x and an exponent that does depend on x.

00:03:43.219 --> 00:03:46.310
We do the same thing that we did before which is,

00:03:46.310 --> 00:03:48.289
we take the maximum of the function in

00:03:48.289 --> 00:03:52.414
the exponent by taking the derivative and finding where it's zero.

00:03:52.414 --> 00:03:57.729
The result for this maximum A posteriori estimate still a Gaussian,

00:03:57.729 --> 00:04:01.074
but the Gaussian hasn't updated mean and covariance.

00:04:01.074 --> 00:04:04.364
We first do a covariance update to get Q1,

00:04:04.365 --> 00:04:07.314
and notice that it does depend on R even if we define

00:04:07.314 --> 00:04:10.359
R to be a variance times the identity matrix,

00:04:10.360 --> 00:04:14.124
and then we do an update to the mean based on this new covariance.

00:04:14.123 --> 00:04:18.399
You can think of the maximum A posteriori estimate as a weighted average

00:04:18.399 --> 00:04:22.529
between the prior x hat naught and what the new measurement says about x.

00:04:22.529 --> 00:04:26.884
The weights come from the relative magnitude of the prior covariance, Q naught,

00:04:26.884 --> 00:04:31.105
and the measurement covariance R. The more we trust the prior estimate,

00:04:31.105 --> 00:04:35.509
the smaller the prior covariance and the less the measurements will affect the estimate.

00:04:35.509 --> 00:04:37.594
The more we trust our measurements,

00:04:37.595 --> 00:04:40.520
the smaller the measurement covariance and the more the

00:04:40.519 --> 00:04:43.774
new measurements will change our current estimate.

00:04:43.774 --> 00:04:48.259
As I've said once before and we'll say a few more times for this module,

00:04:48.259 --> 00:04:50.599
that balance between prior information and

00:04:50.600 --> 00:04:54.120
new information is the heart of the estimation process.

