WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.259
Now, we've assumed so far that our random variables are scalars,

00:00:04.259 --> 00:00:06.870
but what if they are actually vectors.

00:00:06.870 --> 00:00:10.429
The vehicle position isn't going to be given by a single variable x,

00:00:10.429 --> 00:00:13.265
It's going to be given by x, y and z.

00:00:13.265 --> 00:00:16.410
Presumably, we could fold attitude in there as well.

00:00:16.410 --> 00:00:18.570
Which would make x, a six dimensional vector.

00:00:18.570 --> 00:00:22.329
And we'd like to have a distribution over those variables.

00:00:22.329 --> 00:00:24.699
If we treat each variable separately,

00:00:24.699 --> 00:00:26.759
then we're making some strong assumptions about

00:00:26.760 --> 00:00:29.705
independence that we'll talk about in a little bit.

00:00:29.704 --> 00:00:35.104
But, we can actually define vector value functions over the vector value inputs.

00:00:35.104 --> 00:00:38.869
And the same concepts that we saw for scalar values existence before.

00:00:38.869 --> 00:00:41.750
The integral still has to come to one.

00:00:41.750 --> 00:00:44.310
In this case, the normalization constant that makes this

00:00:44.310 --> 00:00:46.760
come to one would be the integral of f(x),

00:00:46.759 --> 00:00:50.159
dx1, dx2 up to dxn.

00:00:50.159 --> 00:00:53.099
The mean it's given by the same set of integrals,

00:00:53.100 --> 00:00:56.520
but it's the integral of x times f(x).

00:00:56.520 --> 00:00:57.874
Instead of a variance,

00:00:57.874 --> 00:00:59.109
we have a co-variance,

00:00:59.109 --> 00:01:01.679
which is going to be given in this case by upper case

00:01:01.679 --> 00:01:06.045
Sigma and that's going to be equal to the integral of x minus mu,

00:01:06.045 --> 00:01:09.829
times x minus mu transpose, times the density.

00:01:09.829 --> 00:01:12.840
This co-variance is going to be a matrix,

00:01:12.840 --> 00:01:16.299
because the outer product is going to result in a matrix as well.

00:01:16.299 --> 00:01:20.849
The co-variance is going to be a symmetric matrix and positive semi-definite.

00:01:20.849 --> 00:01:22.789
If you remember your linear algebra,

00:01:22.790 --> 00:01:27.360
positive semi-definite means that there are no negative eigenvalues.

00:01:27.359 --> 00:01:31.185
Now, we can define a multivariate Gaussian.

00:01:31.185 --> 00:01:34.379
We have to again take care to do the vector math correctly,

00:01:34.379 --> 00:01:37.364
but the functional form is not that different.

00:01:37.364 --> 00:01:40.199
We're no longer taking square root of two pi,

00:01:40.200 --> 00:01:43.105
it's n over two for a vector x of length n.

00:01:43.105 --> 00:01:45.855
This term here is the determinant to the co-variance

00:01:45.855 --> 00:01:49.260
and then we have the vector value multiplication.

00:01:49.260 --> 00:01:52.950
The estimate of the parameters is almost the same as before,

00:01:52.950 --> 00:01:55.085
taking care to do the vector math correctly.

00:01:55.084 --> 00:01:58.904
And we see here an example of a two dimensional co-variance.

00:01:58.905 --> 00:02:05.370
And an important property of the co-variance, is that the eigenvalues and eigenvectors of the co-variance,

00:02:05.370 --> 00:02:09.064
describe the amount and direction of uncertainty.

00:02:09.064 --> 00:02:12.180
Here, we have an overhead view and

00:02:12.180 --> 00:02:15.510
that location at the center of the ellipse would be the mean.

00:02:15.509 --> 00:02:20.159
If we take the co-variance and extract the eigenvalues and eigenvectors,

00:02:20.159 --> 00:02:24.990
this would be the first eigenvector and this will be the second eigenvector.

00:02:24.990 --> 00:02:29.955
If I draw an ellipse at one eigenvalue away from the mean along each vector,

00:02:29.955 --> 00:02:32.820
then this one eigenvalue ellipse is going to define

00:02:32.819 --> 00:02:36.254
a confidence region which in this case is an ellipsoid.

00:02:36.254 --> 00:02:39.449
Just as one standard deviation distance away from the mean and

00:02:39.449 --> 00:02:43.409
the scalar case defined the 68 percent confidence interval for x,

00:02:43.409 --> 00:02:46.280
the ellipse in 2D defines the confidence interval.

00:02:46.280 --> 00:02:48.569
The total probability inside the ellipse at

00:02:48.569 --> 00:02:51.329
one standard deviation is not 68 percent in this case.

00:02:51.330 --> 00:02:53.719
The total probability for a given length of

00:02:53.719 --> 00:02:57.629
axis actually depends on the number of variables in the random vector.

00:02:57.629 --> 00:03:02.085
But in 2D, 95 percent probability is captured by axis with length,

00:03:02.085 --> 00:03:04.879
roughly five times the eigenvalues.

